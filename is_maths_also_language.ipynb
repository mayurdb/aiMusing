{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a831c235-5f46-4311-a262-90287a99f0e4",
   "metadata": {},
   "source": [
    "### Curious case of arithmetic in non-english\n",
    "\n",
    "Since it seems that the LLMs are internally translating to english even for non-english related tasks, I wonder what effect it would have on its mathematical aptitude in non-english language? Is the internal english phenomenon true for english as well? \n",
    "\n",
    "Based on the intuition, even maths is learnt as an assosciation of tokens and thereby in english.\n",
    "\n",
    "But let's find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5412700-3116-411b-a1b7-cc43d0984ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "247b1025-2c3e-441e-9397-19b4ec8162c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from helpers.llamawrapper import LlamaHelper\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "from helpers.utils import plot_ci, plot_ci_plus_heatmap\n",
    "from tqdm import tqdm\n",
    "\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "hf_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a11e5d-abe7-4f64-8f03-e538ab8b08bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayurb/src/open/ai/scrape311/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/mayurb/src/open/ai/scrape311/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e18fddfb994c2b9ae46608e4c85243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_model = '/Users/mayurb/src/open/llm-latent-language/Llama-2-7B-hf'\n",
    "if custom_model is not None:\n",
    "    llama = LlamaHelper(dir=custom_model, load_in_8bit=False, hf_token=hf_token, device=\"mps\")\n",
    "    # llama = LlamaHelper(dir=custom_model, load_in_8bit=True, hf_token=hf_token)\n",
    "    # For GPU use this^\n",
    "else:\n",
    "    llama = LlamaHelper(dir='/dlabdata1/llama2_hf/Llama-2-%s-hf'%model_size, load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aecd63da-f90a-43f4-a3f5-f53839d0e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llama.tokenizer\n",
    "model = llama.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab137ba-b95e-4f61-aaca-b9e1eae770e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unemb = nn.Sequential(llama.model.model.norm, llama.model.lm_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd3a1f26-62ec-44cc-8db7-4d75aaee9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ans(prompt, max_length=100, max_new_tokens=100):\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        generate_ids = model.generate(inputs.input_ids.to('cpu'), max_length=max_length,max_new_tokens=max_new_tokens)\n",
    "        return tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "def generate_next_token(prompt):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate the next token\n",
    "    generate_ids = model.generate(\n",
    "        inputs.input_ids.to('cpu'), \n",
    "        max_new_tokens=1,  # Generate only one new token\n",
    "        do_sample=True,    # This enables sampling to avoid always generating the most likely token\n",
    "    )\n",
    "    \n",
    "    # Decode the generated token\n",
    "    next_token = tokenizer.batch_decode(generate_ids[:, -1:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89299d76-f870-4bc7-b767-90f31dde6f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Both `max_new_tokens` (=5) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'22 + 3 = 25.\\n2'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ans(\"22 + 3 = \", max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cb8d66a-3c23-45e4-bcbc-6ecfee4f81aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Both `max_new_tokens` (=10) and `max_length`(=100) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'२२ + ३ = ४'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ans(\"२२ + ३ = \", max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acac2ab0-af3c-42f6-a513-edd528e0303b",
   "metadata": {},
   "source": [
    "So it look like seemingly simple addition works in english but not in devnagari alhabets.\n",
    "Let's find out what the hidden states are representing in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa45c245-4e9c-4c18-be5e-235aab79c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_token_from_hidden_states(prompt):\n",
    "    latents = llama.latents_all_layers(prompt)\n",
    "    logits = unemb(latents)\n",
    "    last_token_states = logits[:, -1, :].float().softmax(dim=-1).detach().cpu()\n",
    "    top_k_tokens = []\n",
    "    top_k_probs = []\n",
    "    for layer_id in range(last_token_states.shape[0]):\n",
    "        layer = last_token_states[layer_id]\n",
    "        top_probs, top_indices = torch.topk(layer, k=10)\n",
    "        top_words = [tokenizer.decode([idx.item()]) for idx in top_indices]\n",
    "        top_k_tokens.append(top_words)\n",
    "        top_k_probs.append(top_probs.tolist())\n",
    "        # print(f\"layer {layer_id}: {top_words}\")\n",
    "    return top_k_tokens, top_k_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1adad429-a7ce-4b01-8734-f59b30416f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_k_probabilities_heatmap(top_k_tokens, top_k_probs, script, num_layers=32, k=10):\n",
    "    fig, ax = plt.subplots(figsize=(14, num_layers * 0.5))\n",
    "    \n",
    "    top_k_probs_array = np.array(top_k_probs)\n",
    "    \n",
    "    top_k_probs_normalized = top_k_probs_array / top_k_probs_array.max(axis=1, keepdims=True)\n",
    "\n",
    "    cax = ax.imshow(top_k_probs_normalized, cmap='viridis', aspect='auto', interpolation='nearest')\n",
    "\n",
    "    cbar = fig.colorbar(cax, orientation='horizontal', location='top')\n",
    "    cbar.set_label('Probability Increasing ---->')\n",
    "    cbar.set_ticks([])\n",
    "\n",
    "    ax.set_yticks(np.arange(num_layers))\n",
    "    ax.set_yticklabels([f'Layer {i}' for i in range(num_layers)], fontsize=10)\n",
    "\n",
    "    ax.set_xticks(np.arange(k))\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        for j in range(k):\n",
    "            prob_value = top_k_probs_normalized[i, j]\n",
    "            text_color = 'black' if prob_value > 0.5 else 'white'\n",
    "            ax.text(j, i, top_k_tokens[i][j], ha='center', va='center', color=text_color, fontsize=12)\n",
    "\n",
    "    ax.set_xlabel('Top k Tokens')\n",
    "    ax.set_ylabel('Layers')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.title(script)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
